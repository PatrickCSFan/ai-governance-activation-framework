# ðŸ›¡ï¸ What Is AI Safety?

**AI Safety** focuses on ensuring that AI systemsâ€”especially advanced or autonomous onesâ€”**donâ€™t cause harm** to individuals, organizations, or society.

It asks the question:  
> â€œCould this AI system behave in dangerous or unpredictable ways?â€

---

## âš ï¸ Why Itâ€™s Important

Even helpful AI systems can:
- Make mistakes at scale (e.g. faulty automation)
- Be misused (e.g. generating fake news or malware)
- Fail in surprising ways (e.g. hallucinating false outputs)

AI safety helps prevent:
- **Physical harm** (e.g. robotics, autonomous driving)
- **Societal harm** (e.g. bias, disinformation)
- **Security threats** (e.g. prompt injection or jailbreaking)

---

## ðŸ§ª Key AI Safety Practices

| Practice                    | Example |
|----------------------------|---------|
| **Red teaming**            | Testing LLMs for dangerous behaviors |
| **Adversarial testing**    | Probing for manipulations or exploitability |
| **Failsafes & monitoring** | Catching runaway behavior or misuse early |
| **Access controls**        | Restricting model capabilities (e.g. who can prompt or deploy) |

> Learn more from [Anthropic on AI Safety](https://www.anthropic.com/index/2023/10/04/red-teaming-language-models) and [Google DeepMind Safety](https://deepmind.google/technical-blog/safe-ai/)

---

## ðŸ”„ AI Safety vs. AI Alignment

| Term           | Focus Area                     |
|----------------|--------------------------------|
| **AI Alignment** | Make AI do what we want        |
| **AI Safety**     | Make sure it doesn't do harm   |

You can have **aligned but unsafe** systems (e.g. following instructions too literally).  
You can also have **safe but misaligned** systems (e.g. filtered but unhelpful).

---

## ðŸ“Ž Related Concepts

- [Responsible AI](./responsible-ai.md)
- [AI Governance](./ai-governance.md)
- [AI Alignment](./ai-alignment.md)
- [Comparison Table](./comparisons.md)
