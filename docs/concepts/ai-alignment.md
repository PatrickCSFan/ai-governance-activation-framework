# ðŸŽ¯ What Is AI Alignment?

**AI Alignment** means making sure that an AI systemâ€™s behavior and outputs stay in line with **human intentions, values, and goals**.

In other words:  
> â€œDoes this AI do what we actually want it to doâ€”and not something harmful or unintended?â€

---

## ðŸ§  Why It Matters (Especially for LLMs)

LLMs like GPT-4 or Claude are trained on massive data, but they:
- Donâ€™t â€œunderstandâ€ human values by default
- May confidently generate biased, false, or risky content
- Can be steered (aligned) through prompts, tuning, and rules

Without alignment, even well-intentioned systems can:
- Say inappropriate or offensive things
- Give misleading advice
- Amplify social or cultural bias

---

## ðŸ”§ Common AI Alignment Techniques

| Technique                  | Example Use Case |
|---------------------------|------------------|
| **Reinforcement Learning from Human Feedback (RLHF)** | Training GPT-style models to give helpful answers |
| **Constitutional AI**     | Using principles to guide behavior (e.g. Anthropicâ€™s Claude) |
| **Prompt Engineering**    | Designing clear instructions to shape model outputs |
| **Guardrails & Filters**  | Blocking unsafe responses (e.g. profanity, hate speech) |

> Learn more from [OpenAI on Alignment](https://openai.com/research/alignment) and [Anthropicâ€™s Constitutional AI](https://www.anthropic.com/index/2023/08/13/constitutional-ai)

---

## ðŸ§© How Itâ€™s Different from Governance

- **Alignment** is about model behavior  
- **Governance** is about system oversight and accountability

They work best **together**:
- Use governance to decide *what* the system should do
- Use alignment to ensure the AI *actually does it*

---

## ðŸ“Ž Related Concepts

- [Responsible AI](./responsible-ai.md)
- [AI Governance](./ai-governance.md)
- [AI Safety](./ai-safety.md)
- [Comparison Table](./comparisons.md)
