# 🎯 What Is AI Alignment?

**AI Alignment** means making sure that an AI system’s behavior and outputs stay in line with **human intentions, values, and goals**.

In other words:  
> “Does this AI do what we actually want it to do—and not something harmful or unintended?”

---

## 🧠 Why It Matters (Especially for LLMs)

LLMs like GPT-4 or Claude are trained on massive data, but they:
- Don’t “understand” human values by default
- May confidently generate biased, false, or risky content
- Can be steered (aligned) through prompts, tuning, and rules

Without alignment, even well-intentioned systems can:
- Say inappropriate or offensive things
- Give misleading advice
- Amplify social or cultural bias

---

## 🔧 Common AI Alignment Techniques

| Technique                  | Example Use Case |
|---------------------------|------------------|
| **Reinforcement Learning from Human Feedback (RLHF)** | Training GPT-style models to give helpful answers |
| **Constitutional AI**     | Using principles to guide behavior (e.g. Anthropic’s Claude) |
| **Prompt Engineering**    | Designing clear instructions to shape model outputs |
| **Guardrails & Filters**  | Blocking unsafe responses (e.g. profanity, hate speech) |

> Learn more from [OpenAI on Alignment](https://openai.com/research/alignment) and [Anthropic’s Constitutional AI](https://www.anthropic.com/index/2023/08/13/constitutional-ai)

---

## 🧩 How It’s Different from Governance

- **Alignment** is about model behavior  
- **Governance** is about system oversight and accountability

They work best **together**:
- Use governance to decide *what* the system should do
- Use alignment to ensure the AI *actually does it*

---

## 📎 Related Concepts

- [Responsible AI](./responsible-ai.md)
- [AI Governance](./ai-governance.md)
- [AI Safety](./ai-safety.md)
- [Comparison Table](./comparisons.md)
